{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case-Explainer Tutorial: Iris Dataset\n",
    "\n",
    "Welcome to this tutorial on **case-based explainability** using the classic Iris dataset!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to create case-based explanations for ML predictions\n",
    "- Understanding the correspondence metric\n",
    "- Interpreting explanations through similar training examples\n",
    "- Analyzing prediction confidence via correspondence scores\n",
    "\n",
    "## What is Case-Based Explainability?\n",
    "\n",
    "Instead of saying *\"Feature X has importance 0.45\"*, case-based explainability answers:\n",
    "\n",
    "> **\"This prediction was made because it resembles these 5 training examples\"**\n",
    "\n",
    "This approach is particularly valuable when:\n",
    "- Domain experts need to verify predictions against known cases\n",
    "- You want concrete examples rather than abstract feature importances\n",
    "- The training data has meaningful context worth surfacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, let's import the required libraries and load the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Add case_explainer to path\n",
    "sys.path.insert(0, '..')\n",
    "from case_explainer import CaseExplainer\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = list(data.feature_names)\n",
    "class_names = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(X)} total samples\")\n",
    "print(f\"Training: {len(X_train)} samples\")\n",
    "print(f\"Testing: {len(X_test)} samples\")\n",
    "print(f\"\\nFeatures: {feature_names}\")\n",
    "print(f\"Classes: {list(class_names.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Classifier\n",
    "\n",
    "We'll train a Random Forest classifier. Case-explainer works with any classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.1%}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=list(class_names.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the CaseExplainer\n",
    "\n",
    "Now we initialize the explainer with our training data. It builds a k-NN index for fast lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create explainer\n",
    "explainer = CaseExplainer(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    algorithm='kd_tree',  # Fast for low-dimensional data\n",
    "    scale_data=True       # Standardize features\n",
    ")\n",
    "\n",
    "print(\"\\nExplainer initialized!\")\n",
    "print(f\"Index built on {len(X_train)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explain a Single Prediction\n",
    "\n",
    "Let's explain a test sample and see which training examples it resembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a test sample\n",
    "test_idx = 0\n",
    "test_sample = X_test[test_idx]\n",
    "true_label = y_test[test_idx]\n",
    "\n",
    "# Get explanation\n",
    "explanation = explainer.explain_instance(\n",
    "    test_sample=test_sample,\n",
    "    k=5,  # Find 5 nearest neighbors\n",
    "    model=clf,\n",
    "    true_class=true_label\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(explanation.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Correspondence Score\n",
    "\n",
    "The **correspondence** quantifies agreement between the prediction and nearest neighbors:\n",
    "\n",
    "- **High (≥85%)**: Strong agreement, high confidence\n",
    "- **Medium (70-85%)**: Moderate agreement\n",
    "- **Low (<70%)**: Weak agreement, uncertain prediction\n",
    "\n",
    "Let's examine the neighbors in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prediction: {class_names[explanation.predicted_class]}\")\n",
    "print(f\"True class: {class_names[explanation.true_class]}\")\n",
    "print(f\"Correspondence: {explanation.correspondence:.1%} ({explanation.correspondence_interpretation})\")\n",
    "print(f\"Correct: {explanation.is_correct()}\")\n",
    "\n",
    "print(\"\\nNearest Neighbors:\")\n",
    "for i, neighbor in enumerate(explanation.neighbors, 1):\n",
    "    print(f\"\\n{i}. Training sample #{neighbor.index}\")\n",
    "    print(f\"   Distance: {neighbor.distance:.3f}\")\n",
    "    print(f\"   Label: {class_names[neighbor.label]}\")\n",
    "    print(f\"   Features: {neighbor.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the Explanation\n",
    "\n",
    "The plot shows neighbor distances colored by their class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "explanation.plot()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Explanations\n",
    "\n",
    "Let's explain all test samples and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain all test samples\n",
    "explanations = explainer.explain_batch(\n",
    "    X_test=X_test,\n",
    "    k=5,\n",
    "    y_test=y_test,\n",
    "    model=clf\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(explanations)} explanations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Correspondence Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract correspondence scores\n",
    "correspondences = [exp.correspondence for exp in explanations]\n",
    "correct_corr = [exp.correspondence for exp in explanations if exp.is_correct()]\n",
    "incorrect_corr = [exp.correspondence for exp in explanations if not exp.is_correct()]\n",
    "\n",
    "print(\"Correspondence Statistics:\")\n",
    "print(f\"  Overall mean: {np.mean(correspondences):.1%}\")\n",
    "print(f\"  Overall std: {np.std(correspondences):.1%}\")\n",
    "print(f\"\\n  Correct predictions: {np.mean(correct_corr):.1%}\")\n",
    "if incorrect_corr:\n",
    "    print(f\"  Incorrect predictions: {np.mean(incorrect_corr):.1%}\")\n",
    "else:\n",
    "    print(f\"  Incorrect predictions: None (perfect accuracy!)\")\n",
    "\n",
    "# Count by interpretation\n",
    "high = sum(1 for exp in explanations if exp.correspondence_interpretation == 'high')\n",
    "medium = sum(1 for exp in explanations if exp.correspondence_interpretation == 'medium')\n",
    "low = sum(1 for exp in explanations if exp.correspondence_interpretation == 'low')\n",
    "\n",
    "print(f\"\\nCorrespondence Levels:\")\n",
    "print(f\"  High (≥85%): {high} samples\")\n",
    "print(f\"  Medium (70-85%): {medium} samples\")\n",
    "print(f\"  Low (<70%): {low} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Correspondence Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(correspondences, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.mean(correspondences), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(correspondences):.1%}')\n",
    "plt.xlabel('Correspondence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Correspondence Scores')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Correct vs Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare correspondence for correct vs incorrect\n",
    "if incorrect_corr:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot([correct_corr, incorrect_corr], labels=['Correct', 'Incorrect'])\n",
    "    plt.ylabel('Correspondence Score')\n",
    "    plt.title('Correspondence: Correct vs Incorrect Predictions')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nInsight: Correct predictions have {np.mean(correct_corr):.1%} correspondence\")\n",
    "    print(f\"         Incorrect predictions have {np.mean(incorrect_corr):.1%} correspondence\")\n",
    "    print(f\"         Difference: {np.mean(correct_corr) - np.mean(incorrect_corr):.1%}\")\n",
    "else:\n",
    "    print(\"All predictions were correct! No incorrect predictions to compare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect Low Correspondence Predictions\n",
    "\n",
    "Predictions with low correspondence might be uncertain or unusual cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find predictions with lowest correspondence\n",
    "sorted_exps = sorted(explanations, key=lambda x: x.correspondence)\n",
    "lowest_5 = sorted_exps[:5]\n",
    "\n",
    "print(\"5 Predictions with Lowest Correspondence:\\n\")\n",
    "for i, exp in enumerate(lowest_5, 1):\n",
    "    print(f\"{i}. Correspondence: {exp.correspondence:.1%} ({exp.correspondence_interpretation})\")\n",
    "    print(f\"   Predicted: {class_names[exp.predicted_class]}\")\n",
    "    print(f\"   True: {class_names[exp.true_class]}\")\n",
    "    print(f\"   Correct: {exp.is_correct()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Case-based explanations** provide concrete training examples as evidence\n",
    "2. **Correspondence** quantifies how well neighbors agree with the prediction\n",
    "3. **High correspondence** indicates confident predictions backed by similar training data\n",
    "4. **Low correspondence** suggests unusual cases or uncertain predictions\n",
    "5. **Correct predictions** generally have higher correspondence than incorrect ones\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Try the other tutorials:\n",
    "- `02_breast_cancer_tutorial.ipynb` - Medical diagnosis domain\n",
    "- `03_fraud_detection_tutorial.ipynb` - Financial domain with imbalanced data\n",
    "- `04_hardware_trojan_tutorial.ipynb` - Security domain with large-scale data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
